import pandas as pd
import numpy as np
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from sklearn.metrics import ndcg_score
import os
import gc

# å¼•å…¥å°ˆæ¡ˆæ¨¡çµ„
import db_manager
import feature_utils
import adaptive_system

# --- è¨­å®š ---
MODEL_FILE = 'ptt_lifecycle_model.txt'
HORIZONS = range(3, 13) # 3 åˆ° 12 å°æ™‚
EVAL_DAYS = 3           # ä½¿ç”¨æœ€è¿‘ 3 å¤©çš„è³‡æ–™ä¾†è©•ä¼°
TOLERANCE_MINUTES = 30  # é…å°å®¹è¨±èª¤å·®

def compute_ndcg(y_true, y_score, k=10):
    y_true = np.asarray([y_true])
    y_score = np.asarray([y_score])
    if k > y_true.shape[1]: k = y_true.shape[1]
    return ndcg_score(y_true, y_score, k=k) if k > 0 else 0.0

def load_data_pool(days=3):
    print(f"ğŸ“‚ è¼‰å…¥æœ€è¿‘ {days} å¤©çš„è³‡æ–™ä½œç‚ºè©•ä¼°æ± ...")
    end_time = datetime.now()
    start_time = end_time - timedelta(days=days)
    df = db_manager.query_snapshots_by_time_range(start_time, end_time)
    print(f"   -> å–å¾— {len(df)} ç­†è³‡æ–™")
    return df

def evaluate_horizon(model, df_pool, hours):
    print(f"   Testing Horizon: {hours} hours...")
    
    # å»ºç«‹é…å°
    df_pool = df_pool.sort_values('crawl_time')
    df_pool['target_time'] = df_pool['crawl_time'] + pd.Timedelta(hours=hours)
    
    df_future = df_pool[['Post_ID', 'crawl_time', 'push_count', 'boo_count']].copy()
    
    # 1. åŸ·è¡Œæœªä¾†é…å° (T + H)
    merged = pd.merge_asof(
        df_pool,
        df_future,
        left_on='target_time',
        right_on='crawl_time',
        by='Post_ID',
        tolerance=pd.Timedelta(minutes=TOLERANCE_MINUTES),
        direction='nearest',
        suffixes=('', '_future')
    )
    
    # ç§»é™¤ç„¡æ•ˆè³‡æ–™
    valid_data = merged.dropna(subset=['push_count_future']).copy()
    if len(valid_data) < 100:
        print(f"      âš ï¸ æ¨£æœ¬ä¸è¶³ ({len(valid_data)}), è·³é")
        return None

    # 2. åŸ·è¡Œéå»é…å° (T - 10min) ç”¨æ–¼è¨ˆç®—é€Ÿåº¦
    valid_data['prev_time'] = valid_data['crawl_time'] - pd.Timedelta(minutes=10)
    df_prev_lookup = df_pool[['Post_ID', 'crawl_time', 'push_count']].copy()
    
    valid_with_prev = pd.merge_asof(
        valid_data.sort_values('crawl_time'),
        df_prev_lookup.sort_values('crawl_time'),
        left_on='prev_time',
        right_on='crawl_time',
        by='Post_ID',
        tolerance=pd.Timedelta(minutes=10),
        suffixes=('', '_prev')
    )
    
    # ğŸš¨ [é—œéµä¿®æ­£] é‡ç½®ç´¢å¼•ï¼Œç¢ºä¿è³‡æ–™åˆ—é †åºæ˜¯ 0, 1, 2...
    # é€™æ¨£å¾ŒçºŒç”¨ groupby å–å¾—çš„ç´¢å¼•å°±èƒ½ç›´æ¥å°æ‡‰åˆ° numpy array
    valid_with_prev = valid_with_prev.reset_index(drop=True)
    
    # è¨ˆç®—ç‰¹å¾µ (Velocity)
    t_now = valid_with_prev['crawl_time']
    t_prev = valid_with_prev['crawl_time_prev']
    valid_with_prev['time_diff'] = (t_now - t_prev).dt.total_seconds() / 60
    valid_with_prev['push_diff'] = valid_with_prev['push_count'] - valid_with_prev['push_count_prev']
    valid_with_prev['push_velocity'] = valid_with_prev['push_diff'] / valid_with_prev['time_diff']
    valid_with_prev['push_velocity'] = valid_with_prev['push_velocity'].fillna(0)
    
    # æº–å‚™æ¨¡å‹ç‰¹å¾µ
    X = feature_utils.prepare_features_for_model(valid_with_prev)
    X['push_velocity'] = valid_with_prev['push_velocity'] # è¦†è“‹ç¢ºä¿æ­£ç¢º
    
    # é æ¸¬
    preds = model.predict(X)
    
    # è¨ˆç®— Ground Truth
    raw_score = valid_with_prev['push_count_future'] + valid_with_prev['boo_count_future']
    y_true = np.floor(5 * np.log1p(raw_score)).astype(int).clip(0, 30)
    
    # è¨ˆç®— NDCG
    ndcg_list = []
    
    # Group by crawl_time (é€™æ™‚ group_idx æ˜¯æ•´æ•¸ç´¢å¼•)
    grouped = valid_with_prev.groupby('crawl_time')
    
    for name, group_idx in grouped.groups.items():
        if len(group_idx) < 5: continue
        
        # å› ç‚ºå‰é¢åšäº† reset_indexï¼Œé€™è£¡å¯ä»¥ç›´æ¥ç”¨æ•´æ•¸ç´¢å¼•å–å€¼
        g_y_true = y_true.iloc[group_idx].values
        g_preds = preds[group_idx]
        
        s = compute_ndcg(g_y_true, g_preds, k=10)
        ndcg_list.append(s)
        
    if not ndcg_list:
        return 0.0
        
    avg_ndcg = np.mean(ndcg_list)
    print(f"      -> Avg NDCG@10: {avg_ndcg:.4f}")
    
    return avg_ndcg

def main():
    print("ğŸš€ å•Ÿå‹•é•·ç¨‹é æ¸¬è©•ä¼°ç³»çµ± (3hr - 12hr)...")
    
    if not os.path.exists(MODEL_FILE):
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ")
        return

    model = lgb.Booster(model_file=MODEL_FILE)
    df_pool = load_data_pool(days=EVAL_DAYS)
    
    if df_pool.empty:
        print("âŒ ç„¡è³‡æ–™å¯ä¾›è©•ä¼°")
        return

    results = []
    
    print("\nğŸ“Š é–‹å§‹è©•ä¼°å„æ™‚æ®µæº–ç¢ºåº¦...")
    for h in HORIZONS:
        score = evaluate_horizon(model, df_pool, h)
        if score is not None:
            results.append({'Horizon (Hours)': h, 'NDCG@10': score})
            
    # ç¹ªåœ–
    if results:
        res_df = pd.DataFrame(results)
        print("\nğŸ“ˆ è©•ä¼°çµæœ:")
        print(res_df)
        
        plt.figure(figsize=(10, 6))
        sns.lineplot(data=res_df, x='Horizon (Hours)', y='NDCG@10', marker='o')
        plt.title('Model Accuracy Decay over Time (3-12 Hours)')
        plt.ylim(0.5, 1.0)
        plt.grid(True)
        plt.savefig('long_term_accuracy.png')
        print("âœ… åœ–è¡¨å·²å„²å­˜è‡³ long_term_accuracy.png")
        
        # é æ¸¬æœªä¾†
        print("\nğŸ”® [å³æ™‚é æ¸¬] åŸºæ–¼æœ€æ–°è³‡æ–™é æ¸¬æœªä¾†...")
        latest_df, latest_time = adaptive_system.load_latest_snapshot_from_db()
        if latest_df is not None:
            # å–å¾— T-10 è¨ˆç®—é€Ÿåº¦
            t_prev = latest_time - timedelta(minutes=10)
            df_prev, _ = db_manager.query_nearest_snapshot(t_prev)
            
            X_latest = feature_utils.prepare_features_for_model(latest_df, df_prev)
            scores = model.predict(X_latest)
            
            latest_df['pred_score'] = scores
            top10 = latest_df.sort_values('pred_score', ascending=False).head(10)
            
            print(f"è³‡æ–™æ™‚é–“: {latest_time.strftime('%Y-%m-%d %H:%M')}")
            print("-" * 60)
            print(f"{'é æ¸¬æ’å':<5} | {'ç›®å‰æ¨æ•¸':<8} | {'æ¨™é¡Œ'}")
            print("-" * 60)
            for i, row in enumerate(top10.itertuples()):
                print(f"#{i+1:<4} | {row.push_count:<8} | {row.title}")
            print("-" * 60)
            print("(è¨»ï¼šé€™æ˜¯æ¨¡å‹èªç‚ºåœ¨æœªä¾† 3~12 å°æ™‚å…§æœ€å…·ç«¶çˆ­åŠ›çš„æ–‡ç« )")

if __name__ == "__main__":
    main()