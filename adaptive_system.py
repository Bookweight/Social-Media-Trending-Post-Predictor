import time
import pandas as pd
import numpy as np
import os
import lightgbm as lgb
from datetime import datetime, timedelta
from sklearn.metrics import mean_squared_error, ndcg_score
import math
import csv
import matplotlib.pyplot as plt
import seaborn as sns

# å¼•å…¥æ¨¡çµ„
import ptt_moniter 
import feature_utils 
import train_model_lifecycle
import db_manager  # å¼•å…¥è³‡æ–™åº«æ¨¡çµ„

# --- è¨­å®š ---
MODEL_FILE = 'ptt_lifecycle_model.txt'
VERSION_FILE = 'model_version.txt'
PRED_LOG_FILE = 'pred.csv'
PLOT_DIR = 'results'
LOOK_AHEAD_MINUTES = 120  # é æ¸¬ 2 å°æ™‚å¾Œ
VELOCITY_DELTA_MINUTES = 10 

def get_current_version():
    if os.path.exists(VERSION_FILE):
        try:
            with open(VERSION_FILE, 'r') as f:
                return int(f.read().strip())
        except:
            return 1
    return 1

def increment_version():
    v = get_current_version() + 1
    with open(VERSION_FILE, 'w') as f:
        f.write(str(v))
    print(f"ğŸ†™ æ¨¡å‹ç‰ˆæœ¬å·²å‡ç´šç‚º v{v}")
    return v

# ğŸ†• [DB] å¾è³‡æ–™åº«è®€å–æœ€æ–°å¿«ç…§
def load_latest_snapshot_from_db():
    conn = db_manager.get_conn()
    cursor = conn.cursor()
    # æ‰¾æœ€æ–°çš„çˆ¬èŸ²æ™‚é–“
    cursor.execute("SELECT MAX(crawl_time) FROM snapshots")
    result = cursor.fetchone()
    
    if not result or not result[0]:
        conn.close()
        return None, None
        
    latest_time_str = result[0]
    # è½‰æ›ç‚º datetime ç‰©ä»¶
    latest_time = pd.to_datetime(latest_time_str)
    
    # è®€å–è©²æ™‚é–“é»çš„æ‰€æœ‰æ–‡ç« 
    # æ³¨æ„ï¼šé€™è£¡ç›´æ¥è®€å–è©²æ™‚é–“é»çš„æ‰€æœ‰è³‡æ–™
    query = "SELECT * FROM snapshots WHERE crawl_time = ?"
    df = pd.read_sql(query, conn, params=(latest_time_str,))
    conn.close()
    
    # ç¢ºä¿æ™‚é–“æ ¼å¼æ­£ç¢º
    if not df.empty:
        df['crawl_time'] = pd.to_datetime(df['crawl_time'])
        if 'post_time' in df.columns:
            df['post_time'] = pd.to_datetime(df['post_time'])
            
    return df, latest_time

def compute_ranking_metrics(y_true, y_score, k=10):
    y_true = np.asarray([y_true])
    y_score = np.asarray([y_score])
    k = min(k, y_true.shape[1])
    if k <= 0: return 0.0, 0.0, 0.0
    try:
        from scipy.stats import kendalltau
        ndcg_10 = ndcg_score(y_true, y_score, k=10)
        ndcg_3 = ndcg_score(y_true, y_score, k=3)
        tau, _ = kendalltau(y_true[0], y_score[0])
        return ndcg_10, ndcg_3, tau
    except:
        return 0.0, 0.0, 0.0

def log_prediction_performance(timestamp, model_metrics, base_metrics, stage="adaptive_verify"):
    file_exists = os.path.isfile(PRED_LOG_FILE)
    
    lift = 0.0
    if base_metrics['ndcg'] > 0:
        lift = (model_metrics['ndcg'] - base_metrics['ndcg']) / base_metrics['ndcg'] * 100

    with open(PRED_LOG_FILE, mode='a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(['timestamp', 'stage', 'model_rmse', 'model_ndcg', 'base_rmse', 'base_ndcg', 'lift_percent'])
            
        writer.writerow([
            timestamp.strftime('%Y-%m-%d %H:%M:%S'),
            stage,
            f"{model_metrics['rmse']:.4f}",
            f"{model_metrics['ndcg']:.4f}",
            f"{base_metrics['rmse']:.4f}",
            f"{base_metrics['ndcg']:.4f}",
            f"{lift:+.2f}%"
        ])
    print(f"ğŸ“ ç¸¾æ•ˆå·²è¨˜éŒ„è‡³ {PRED_LOG_FILE} (Lift: {lift:+.2f}%)")

def print_side_by_side(list_a, list_b, title_a, title_b):
    print("-" * 95)
    print(f"{title_a:<45} | {title_b:<45}")
    print("-" * 95)
    
    for i in range(10):
        str_a, str_b = "", ""
        if i < len(list_a):
            row = list_a.iloc[i]
            title = str(row.title)[:18] + "..." if len(str(row.title)) > 18 else str(row.title)
            score_info = f"[{row.score_val:.1f}]" if 'score_val' in row else f"(æ¨:{row.push_count})"
            str_a = f"#{i+1} {score_info} {title}"

        if i < len(list_b):
            row = list_b.iloc[i]
            title = str(row.title)[:18] + "..." if len(str(row.title)) > 18 else str(row.title)
            score_info = f"[{row.score_val:.1f}]" if 'score_val' in row else f"(æ¨:{row.push_count})"
            str_b = f"#{i+1} {score_info} {title}"
            
        print(f"{str_a:<45} | {str_b:<45}")
    print("-" * 95)

def save_feature_importance_plot(model, timestamp):
    if not os.path.exists(PLOT_DIR):
        os.makedirs(PLOT_DIR)
        
    try:
        importance = model.feature_importance(importance_type='gain')
        feature_name = model.feature_name()
        
        df_importance = pd.DataFrame({
            'feature': feature_name,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        plt.figure(figsize=(12, 8))
        
        # ğŸš¨ [ä¿®æ­£] åŠ å…¥ hue èˆ‡ legend=False ä»¥æ¶ˆé™¤è­¦å‘Š
        sns.barplot(
            x='importance', 
            y='feature', 
            hue='feature', 
            data=df_importance, 
            palette='viridis', 
            legend=False
        )
        
        plt.title(f'Feature Importance (Gain) - {timestamp.strftime("%Y-%m-%d %H:%M")}', fontsize=16)
        plt.xlabel('Gain Score', fontsize=12)
        plt.ylabel('Feature', fontsize=12)
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        
        for i, v in enumerate(df_importance['importance']):
            plt.text(v, i, f' {v:.0f}', va='center', fontsize=9)

        plt.tight_layout()
        filename = f"feature_importance_{timestamp.strftime('%Y%m%d_%H%M')}.png"
        save_path = os.path.join(PLOT_DIR, filename)
        plt.savefig(save_path, dpi=300)
        plt.close()
    except Exception as e:
        print(f"âš ï¸ ç¹ªåœ–å¤±æ•—: {e}")

def calculate_dynamic_weight(df):
    """æ ¹æ“šå…¨ç«™æ¨æ–‡ç¸½é‡ (æµé‡) æ±ºå®š AI çš„æ¬Šé‡"""
    total_push = df['push_count'].sum()
    article_count = len(df)
    
    if article_count == 0: return 0.5
    
    avg_push = total_push / article_count
    
    # Sigmoid function centered at 10
    sigmoid = 1 / (1 + np.exp(-(avg_push - 10) / 3))
    weight = 0.3 + (0.6 * sigmoid)
    
    print(f"âš–ï¸ [å‹•æ…‹æ¬Šé‡] å…¨ç«™å¹³å‡æ¨æ–‡: {avg_push:.1f} -> AI æ¬Šé‡: {weight:.2f}")
    return weight

# ğŸ†• [DB] é æ¸¬é‚è¼¯ (æ¥æ”¶ DataFrame å’Œ æ™‚é–“)
def predict_future_rank(df, current_time, model):
    if df.empty: return

    print(f"\nğŸ”® [é æ¸¬æ¨¡å¼] è³‡æ–™æ™‚é–“: {current_time.strftime('%Y-%m-%d %H:%M')}")
    
    # 1. å¾ DB æ‰¾ T-10 è³‡æ–™ (è¨ˆç®—å‹•èƒ½)
    target_prev_time = current_time - timedelta(minutes=VELOCITY_DELTA_MINUTES)
    df_prev, _ = db_manager.query_nearest_snapshot(target_prev_time)
    
    if df_prev is not None and not df_prev.empty:
        print("   -> æˆåŠŸè¼‰å…¥ T-10min è³‡æ–™ä»¥è¨ˆç®—ç¬æ™‚å‹•èƒ½")
    else:
        print("   -> âš ï¸ ç„¡æ³•è¼‰å…¥ T-10min è³‡æ–™ (å¯èƒ½å‰›å•Ÿå‹•)ï¼Œå‹•èƒ½ç‰¹å¾µå°‡ä½¿ç”¨é è¨­å€¼")

    # 2. æº–å‚™ç‰¹å¾µ
    X = feature_utils.prepare_features_for_model(df, df_prev)
    
    # 3. é æ¸¬
    ai_score = model.predict(X)
    base_score = np.floor(5 * np.log1p(df['push_count'])).clip(0, 30)
    w = calculate_dynamic_weight(df)
    
    df['pred_score'] = (w * ai_score) + ((1-w) * base_score)
    
    # 4. é¡¯ç¤ºçµæœ
    top_pred = df.sort_values('pred_score', ascending=False).head(10).copy()
    top_pred['score_val'] = top_pred['pred_score']
    top_curr = df.sort_values('push_count', ascending=False).head(10).copy()
    
    print(f"ğŸš€ é æ¸¬ {LOOK_AHEAD_MINUTES} åˆ†é˜å¾Œçš„è¶¨å‹¢åˆ†æ")
    print_side_by_side(
        top_pred, top_curr, 
        f"ğŸ¤– AI é æ¸¬æ’å (æ¬Šé‡ {w:.2f})", 
        f"ğŸ”¥ ç›®å‰å¯¦éš›æ’å (ç•¶ä¸‹ç†±åº¦)"
    )

# ğŸ†• [DB] å­¸ç¿’é‚è¼¯
def adaptive_learning(df_now, current_time, model, stage_label):
    # 1. å¾ DB æ‰¾ T-120 (éå»çš„é æ¸¬ç•¶ä¸‹)
    target_time_verify = current_time - timedelta(minutes=LOOK_AHEAD_MINUTES)
    df_past, real_past_time = db_manager.query_nearest_snapshot(target_time_verify, tolerance_seconds=1800)
    
    if df_past is None or df_past.empty:
        print(f"âš ï¸ è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ° T-{LOOK_AHEAD_MINUTES} åˆ†é˜å‰çš„è³‡æ–™ï¼Œè·³éé©—è­‰")
        return model

    print(f"\nğŸ§  [å­¸ç¿’æ¨¡å¼] å›æº¯é©—è­‰: {real_past_time.strftime('%H:%M')} -> {current_time.strftime('%H:%M')}")

    # 2. å¾ DB æ‰¾ T-130 (éå»çš„éå»ï¼Œç‚ºäº†ç®—ç•¶æ™‚çš„é€Ÿåº¦)
    target_time_velocity = real_past_time - timedelta(minutes=VELOCITY_DELTA_MINUTES)
    df_prev_past, _ = db_manager.query_nearest_snapshot(target_time_velocity)

    # 3. åˆä½µé©—è­‰
    merged = pd.merge(df_past, df_now[['Post_ID', 'push_count', 'boo_count']], 
                      on='Post_ID', suffixes=('', '_future'))
    
    if len(merged) < 5: return model

    # 1. æº–å‚™çœŸå¯¦æ¨™ç±¤ (Ground Truth)
    merged['real_future_score'] = merged['push_count_future'] + merged['boo_count_future']
    y_true_grade = np.floor(5 * np.log1p(merged['real_future_score'])).astype(int).clip(0, 30)
    
    # 2. æº–å‚™ç‰¹å¾µèˆ‡é æ¸¬
    X_train = feature_utils.prepare_features_for_model(merged, df_prev_past)
    
    # --- A. AI æ¨¡å‹é æ¸¬ ---
    ai_preds = model.predict(X_train)
    
    # --- B. ç¬¨è›‹åŸºæº– (Stupid Baseline: Rank Freeze) ---
    base_preds = np.floor(5 * np.log1p(merged['push_count'])).clip(0, 30)

    # --- C. [æ–°å¢] å‹•èƒ½åŸºæº– (Velocity Baseline) ---
    # é‚è¼¯: é æ¸¬æ¨æ•¸ = ç›®å‰æ¨æ•¸ + (ç›®å‰é€Ÿåº¦ * é æ¸¬æ™‚é–“é•·åº¦)
    # å¾ X_train å–å‡ºè¨ˆç®—å¥½çš„é€Ÿåº¦ (fillnaé˜²å‘†)
    current_velocity = X_train['push_velocity'].fillna(0)
    
    # ç·šæ€§æ¨æ¼”æœªä¾†æ¨æ–‡æ•¸
    projected_push = merged['push_count'] + (current_velocity * LOOK_AHEAD_MINUTES)
    # é˜²å‘†: æ¨æ–‡æ•¸ä¸æ‡‰æ¸›å°‘
    projected_push = np.maximum(projected_push, merged['push_count'])
    
    # è½‰æˆç­‰ç´šåˆ† (0-30) ä»¥ä¾¿æ¯”è¼ƒ
    vel_preds = np.floor(5 * np.log1p(projected_push)).astype(int).clip(0, 30)

    # 3. æ··åˆæ¬Šé‡è¨ˆç®— (AI + ç¬¨è›‹)
    w = calculate_dynamic_weight(merged)
    mixed_preds = (w * ai_preds) + ((1-w) * base_preds)
    merged['old_pred_score'] = mixed_preds
    
    # 4. æŒ‡æ¨™è¨ˆç®— (Metrics)
    # (1) AI æ··åˆæ¨¡å‹
    model_rmse = math.sqrt(mean_squared_error(y_true_grade, mixed_preds))
    model_ndcg_10, model_ndcg_3, _ = compute_ranking_metrics(y_true_grade, mixed_preds, k=10)   
    
    # (2) ç¬¨è›‹åŸºæº–
    base_eval = np.floor(5 * np.log1p(merged['push_count'])).astype(int).clip(0, 30)
    base_rmse = math.sqrt(mean_squared_error(y_true_grade, base_eval))
    base_ndcg_10, base_ndcg_3, _ = compute_ranking_metrics(y_true_grade, merged['push_count'], k=10)

    # (3) [æ–°å¢] å‹•èƒ½åŸºæº–
    vel_ndcg_10, vel_ndcg_3, _ = compute_ranking_metrics(y_true_grade, vel_preds, k=10)

    # 5. Lift è¨ˆç®—
    lift_base = 0.0
    if base_ndcg_10 > 0:
        lift_base = (model_ndcg_10 - base_ndcg_10) / base_ndcg_10 * 100
        
    lift_vel = 0.0
    if vel_ndcg_10 > 0:
        lift_vel = (model_ndcg_10 - vel_ndcg_10) / vel_ndcg_10 * 100

    # 6. è¼¸å‡ºçµæœ
    print(f"ğŸ“Š é©—è­‰æˆæ•ˆæ¯”è¼ƒ (Hybrid):")
    print(f"   - æ··åˆæ¨¡å‹ : NDCG@10={model_ndcg_10:.4f}, Lift(v.s.ç¬¨è›‹)={lift_base:+.2f}%")
    print(f"   - ç¬¨è›‹åŸºæº– : NDCG@10={base_ndcg_10:.4f}")
    print(f"   - å‹•èƒ½åŸºæº– : NDCG@10={vel_ndcg_10:.4f} | AI v.s. å‹•èƒ½: {lift_vel:+.2f}%")
    
    # å¯«å…¥ Log (ç¶­æŒåŸæ ¼å¼ï¼Œä»¥å…ç ´å£ CSV çµæ§‹ï¼Œä½†æ‚¨å¯ä»¥åœ¨é€™è£¡è€ƒæ…®æ˜¯å¦è¦åŠ æ¬„ä½)
    log_prediction_performance(
        current_time, 
        {'rmse': model_rmse, 'ndcg': model_ndcg_10}, 
        {'rmse': base_rmse, 'ndcg': base_ndcg_10},   
        stage=stage_label 
    )

    top_past_pred = merged.sort_values('old_pred_score', ascending=False).head(10).copy()
    top_past_pred['score_val'] = top_past_pred['old_pred_score']
    top_now_real = merged.sort_values('real_future_score', ascending=False).head(10).copy()
    top_now_real['push_count'] = top_now_real['real_future_score']
    
    print_side_by_side(
        top_past_pred, top_now_real,
        f"ğŸ¤– {LOOK_AHEAD_MINUTES}åˆ†å‰ æ··åˆé æ¸¬ (w={w:.2f})",
        f"âœ… {LOOK_AHEAD_MINUTES}åˆ†å¾Œ çœŸå¯¦çµæœ"
    )

    # 4. å¢é‡è¨“ç·´
    custom_label_gain = [2**i - 1 for i in range(31)]
    group = [len(X_train)]
    lgb_train = lgb.Dataset(X_train, y_true_grade, group=group)
    
    params = {
        'objective': 'lambdarank', 'metric': ['ndcg', 'map'], 'learning_rate': 0.01,
        'num_leaves': 31, 'verbosity': -1, 'lambdarank_truncation_level': 10,
        'label_gain': custom_label_gain
    }
    
    new_model = lgb.train(params, lgb_train, num_boost_round=10, init_model=model, keep_training_booster=True)
    new_model.save_model(MODEL_FILE)
    print("ğŸ’¾ æ¨¡å‹å·²å¾®èª¿ä¸¦å­˜æª”")
    
    return new_model

def smart_start_wait():
    print("ğŸ•µï¸â€â™‚ï¸ [ç³»çµ±æª¢æŸ¥] åµæ¸¬è³‡æ–™åº«æ–°é®®åº¦...")
    df_last, last_time = load_latest_snapshot_from_db()
    
    if last_time is None:
        print("   -> è³‡æ–™åº«ç„¡è³‡æ–™ï¼Œæº–å‚™ç«‹å³å•Ÿå‹•ã€‚")
        return

    current_time = datetime.now()
    elapsed_seconds = (current_time - last_time).total_seconds()
    interval = ptt_moniter.INTERVAL_SECONDS
    
    if 0 <= elapsed_seconds < interval:
        wait_seconds = interval - elapsed_seconds + 5 
        print(f"âœ… æœ€æ–°è³‡æ–™ ({last_time.strftime('%H:%M')}) åƒ…åœ¨ {elapsed_seconds/60:.1f} åˆ†é˜å‰ç”¢ç”Ÿã€‚")
        print(f"â³ ç‚ºé¿å…é‡è¤‡çˆ¬å–ï¼Œç³»çµ±å°‡ä¼‘çœ  {wait_seconds:.0f} ç§’...")
        try:
            time.sleep(wait_seconds)
        except KeyboardInterrupt:
            exit()
    else:
        print(f"âš¡ æœ€æ–°è³‡æ–™å·²æ˜¯ {elapsed_seconds/60:.1f} åˆ†é˜å‰ï¼Œç«‹å³å•Ÿå‹•çˆ¬èŸ²ï¼")

def main_loop():
    print("ğŸš€ PTT è‡ªé©æ‡‰é æ¸¬ç³»çµ±å•Ÿå‹• (è³‡æ–™åº«ç‰ˆ + å‹•æ…‹é˜²ç¦¦)")
    
    # ç¢ºä¿è³‡æ–™åº«åˆå§‹åŒ–
    if not os.path.exists(db_manager.DB_NAME):
        db_manager.init_db()

    if os.path.exists(MODEL_FILE):
        print("ğŸ“‚ è¼‰å…¥ç¾æœ‰æ¨¡å‹...")
        model = lgb.Booster(model_file=MODEL_FILE)
    else:
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹ï¼ŒåŸ·è¡Œåˆå§‹åŒ–è¨“ç·´...")
        # æ³¨æ„: é€™è£¡ train_model_lifecycle ä¹Ÿéœ€è¦æ›´æ–°ç‚ºæ”¯æ´ DB çš„ç‰ˆæœ¬
        train_model_lifecycle.run_training_pipeline(days_back=3)
        model = lgb.Booster(model_file=MODEL_FILE)

    author_cache = ptt_moniter.load_author_history()
    last_plot_hour = -1
    SCHEDULED_HOURS = [0, 6, 12, 18]
    
    last_retrain_time = datetime.now()
    RETRAIN_INTERVAL = timedelta(hours=24) 
    model_version = get_current_version()
    print(f"ğŸ”¢ ç›®å‰æ¨¡å‹ç‰ˆæœ¬: v{model_version}")

    smart_start_wait()

    while True:
        cycle_start_time = datetime.now()
        try:
            print("\n" + "="*95)
            
            # --- é‡è¨“æª¢æŸ¥ ---
            time_since_retrain = datetime.now() - last_retrain_time
            if time_since_retrain > RETRAIN_INTERVAL:
                print(f"â° å·²è·é›¢ä¸Šæ¬¡é‡è¨“ {time_since_retrain}ï¼Œé–‹å§‹åŸ·è¡Œæ¯æ—¥é‡è¨“...")
                success = train_model_lifecycle.run_training_pipeline(days_back=7)
                if success:
                    model = lgb.Booster(model_file=MODEL_FILE)
                    last_retrain_time = datetime.now()
                    model_version = increment_version()
            
            # 1. çˆ¬èŸ² (æœƒè‡ªå‹•å¯«å…¥ DB)
            has_data, _ = ptt_moniter.run_snapshot(author_cache)
            
            # 2. å¾ DB è®€å–
            df_now, current_time = load_latest_snapshot_from_db()
            
            if df_now is not None:
                # ... (é æ¸¬ã€å­¸ç¿’ã€ç¹ªåœ–é‚è¼¯ä¿æŒä¸è®Š) ...
                predict_future_rank(df_now, current_time, model)
                if has_data:
                    model = adaptive_learning(df_now, current_time, model, stage_label=f"adaptive_v{model_version}")
                # ...
            else:
                print("âŒ è³‡æ–™åº«è®€å–å¤±æ•—æˆ–ç„¡è³‡æ–™")
            
            # 3. è¨ˆç®—ä¸‹ä¸€è¼ªçš„ç›®æ¨™æ™‚é–“ (Fixed Rate Scheduling)
            target_next_time = cycle_start_time + timedelta(seconds=ptt_moniter.INTERVAL_SECONDS)
            now = datetime.now()
            sleep_seconds = (target_next_time - now).total_seconds()
            
            if sleep_seconds > 0:
                print(f"âœ… æœ¬è¼ªè€—æ™‚: {(now - cycle_start_time).total_seconds():.1f} ç§’")
                print(f"ğŸ˜´ ç­‰å¾…ä¸­... ä¸‹æ¬¡åŸ·è¡Œ: {target_next_time.strftime('%H:%M:%S')}")
                time.sleep(sleep_seconds)
            else:
                print(f"âš ï¸ è­¦å‘Š: æœ¬è¼ªè€—æ™‚éé•· ({(now - cycle_start_time).total_seconds():.1f} ç§’)ï¼Œç«‹å³å•Ÿå‹•ä¸‹ä¸€è¼ªï¼")
                # ä¸ç¡è¦ºï¼Œç›´æ¥è¶•é€²åº¦

        except KeyboardInterrupt:
            print("ğŸ›‘ åœæ­¢")
            break
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            time.sleep(60) # å‡ºéŒ¯æ™‚ä¼‘æ¯ä¸€ä¸‹

if __name__ == "__main__":
    main_loop()