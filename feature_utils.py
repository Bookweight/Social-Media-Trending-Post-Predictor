import pandas as pd
import numpy as np
import re
import jieba.analyse
from collections import Counter

# --- å…±ç”¨åƒæ•¸ ---
NUMERIC_COLS = [
    'real_push_score', 'push_count', 'boo_count', 'life_minutes', 
    'push_acceleration', 'author_avg_push', 'content_word_count',
    'q_mark_density', 'e_mark_density', 'content_url_ratio', 
    'hour_sin', 'hour_cos', 'is_weekend',
    'is_morning', 'is_noon', 'is_afternoon', 'is_evening', 'is_night',
    'title_hot_score', 
    'push_velocity' # ðŸ†• æ–°å¢ž: 10åˆ†é˜çž¬æ™‚é€Ÿåº¦
]
CAT_COLS = ['category', 'source_board']

STOPWORDS = set([
    'http', 'https', 'com', 'tw', 'imgur', 'jpg', 'jpeg', 'png', 'gif', 
    'youtu', 'be', 'link', 'url', 'æ–°èž', 'åœ–ç‰‡', 'é€£çµ', 'è¨˜è€…', 'å ±å°Ž', 
    'å•é¡Œ', 'å¤§å®¶', 'ä¸€å€‹', 'ä»€éº¼', 'é€™æ¨£', 'å‡ºä¾†', 'æ²’æœ‰', 'å¯ä»¥', 'æ€Žéº¼', 
    'å•å¦', 'çˆ†å¦', 'Re' 
])

def extract_tokens(text):
    if not isinstance(text, str): return []
    words = jieba.cut(text)
    return [w for w in words if len(w) > 1 and w.lower() not in STOPWORDS]

def calculate_hot_score_group(group_df):
    word_heat_map = Counter()
    tokens_list = group_df['title'].apply(extract_tokens).tolist()
    push_counts = group_df['push_count'].tolist()
    
    for tokens, push in zip(tokens_list, push_counts):
        weight = max(1, push) 
        for w in tokens:
            word_heat_map[w] += weight
            
    scores = []
    for tokens in tokens_list:
        if not tokens:
            scores.append(0)
            continue
        article_hot_score = sum(word_heat_map[w] for w in tokens) / len(tokens)
        scores.append(np.log1p(article_hot_score))
    return scores

def prepare_features_for_model(df, df_prev=None):
    """
    å°‡åŽŸå§‹ DataFrame è½‰æ›ç‚ºæ¨¡åž‹å¯åƒçš„ X (ç‰¹å¾µçŸ©é™£)
    åƒæ•¸:
      df: ç•¶å‰çš„å¿«ç…§è³‡æ–™
      df_prev: (é¸ç”¨) 10åˆ†é˜å‰çš„å¿«ç…§è³‡æ–™ï¼Œç”¨æ–¼è¨ˆç®—çž¬æ™‚é€Ÿåº¦
    """
    df_processed = df.copy()
    
    # --- 1. æ™‚é–“ç‰¹å¾µè™•ç† ---
    if 'post_hour' in df_processed.columns:
        h = df_processed['post_hour']
    elif 'post_time' in df_processed.columns:
        h = pd.to_datetime(df_processed['post_time']).dt.hour
    else:
        h = 0 

    df_processed['is_morning']   = ((h >= 6) & (h < 12)).astype(int)
    df_processed['is_noon']      = ((h >= 12) & (h < 14)).astype(int)
    df_processed['is_afternoon'] = ((h >= 14) & (h < 18)).astype(int)
    df_processed['is_evening']   = ((h >= 18) | (h == 0)).astype(int)
    df_processed['is_night']     = ((h >= 1) & (h < 6)).astype(int)

    # --- 2. é—œéµå­—ç†±åº¦è¨ˆç®— ---
    if 'crawl_time' in df_processed.columns:
        df_processed['title_hot_score'] = 0.0
        for _, group in df_processed.groupby('crawl_time'):
            if group.empty: continue
            scores = calculate_hot_score_group(group)
            df_processed.loc[group.index, 'title_hot_score'] = scores
    else:
        df_processed['title_hot_score'] = calculate_hot_score_group(df_processed)

    # å‡è¨­ df å·²ç¶“åŒ…å«äº†è©²æ™‚é–“é»žçš„æ‰€æœ‰æ–‡ç« 
    # å…ˆè¨ˆç®—ç•¶ä¸‹çš„æŽ’å
    df_processed['current_rank'] = df_processed['push_count'].rank(ascending=False)
    
    # è¨ˆç®—èˆ‡ç¬¬ä¸€åçš„æŽ¨æ–‡å·®è·
    max_push = df_processed['push_count'].max()
    df_processed['gap_to_leader'] = max_push - df_processed['push_count']
    
    # è¨ˆç®—èˆ‡ä¸Šä¸€åçš„å·®è· (Gap to Next) - ç«¶çˆ­æ¿€çƒˆåº¦
    # é€™éœ€è¦å…ˆæŽ’åº
    sorted_push = df_processed['push_count'].sort_values(ascending=False)

    # --- 3. 10åˆ†é˜çž¬æ™‚é€Ÿåº¦è¨ˆç®— (ä¿®æ­£ç‰ˆ) ---
    # å„ªå…ˆæª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰ push_count_prev æ¬„ä½ (ä¾†è‡ª train_model_lifecycle çš„ merge)
    if 'push_count_prev' in df_processed.columns and 'crawl_time_prev' in df_processed.columns:
        # ã€æƒ…å¢ƒ Aã€‘è¨“ç·´æ¨¡å¼ï¼šè³‡æ–™å·²ç¶“ merge å¥½äº†
        t_now = pd.to_datetime(df_processed['crawl_time'])
        t_prev = pd.to_datetime(df_processed['crawl_time_prev'])
        time_diff = (t_now - t_prev).dt.total_seconds() / 60
        time_diff = time_diff.replace(0, 10).fillna(10)
        
        push_diff = df_processed['push_count'] - df_processed['push_count_prev']
        df_processed['push_velocity'] = push_diff / time_diff
        
    elif df_prev is not None:
        # ã€æƒ…å¢ƒ Bã€‘é æ¸¬æ¨¡å¼ï¼šå‚³å…¥ç¨ç«‹çš„ df_prev
        prev_data = df_prev[['Post_ID', 'push_count', 'crawl_time']].copy()
        prev_data.columns = ['Post_ID', 'push_count_prev', 'crawl_time_prev']
        
        df_processed = pd.merge(df_processed, prev_data, on='Post_ID', how='left')
        
        t_now = pd.to_datetime(df_processed['crawl_time'])
        t_prev = pd.to_datetime(df_processed['crawl_time_prev'])
        time_diff = (t_now - t_prev).dt.total_seconds() / 60
        time_diff = time_diff.replace(0, 10).fillna(10)
        
        push_diff = df_processed['push_count'] - df_processed['push_count_prev']
        df_processed['push_velocity'] = push_diff / time_diff
        
    else:
        # ã€æƒ…å¢ƒ Cã€‘ç„¡è³‡æ–™
        pass

    # å¡«è£œç¼ºå¤±å€¼ (çµ±ä¸€è™•ç†)
    if 'push_velocity' not in df_processed.columns:
        df_processed['push_velocity'] = df_processed['push_acceleration'] # Fallback
    else:
        df_processed['push_velocity'] = df_processed['push_velocity'].fillna(df_processed['push_acceleration'])

    # --- 4. å¡«è£œç¼ºå¤±å€¼ ---
    for col in NUMERIC_COLS:
        if col not in df_processed.columns: df_processed[col] = 0
        df_processed[col] = df_processed[col].fillna(0)
    
    # --- 5. é¡žåˆ¥ç‰¹å¾µ Hash ---
    for col in CAT_COLS:
        if col in df_processed.columns:
            df_processed[col] = df_processed[col].astype(str).apply(lambda x: hash(x) % 1000)
        else:
            df_processed[col] = 0
            
    return df_processed[NUMERIC_COLS + CAT_COLS]