import requests
import cloudscraper
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import pandas as pd
import time
import random
import os
import glob
import jieba.analyse
import numpy as np

# --- 1. é…ç½®èˆ‡è¨­å®š ---
BOARD = 'Gossiping'
INITIAL_LOOKBACK_HOURS = 24  
REGULAR_LOOKBACK_HOURS = 1   
INTERVAL_SECONDS = 600       
DATA_DIR = 'data'
AUTHOR_HISTORY_FILE = 'data/author_history_recalc.csv' # ğŸ†• æ–°å¢: ä½œè€…æ­·å²çµ±è¨ˆå¿«å–æª”
DEBUG_MODE = False          
CLIPPING_THRESHOLD = 100     

# PTT ç¶²å€èˆ‡ Headers
PTT_BASE_URL = 'https://www.ptt.cc'
PTT_URL = f'{PTT_BASE_URL}/bbs/{BOARD}/index.html'
HEADERS = {
    'User-Agent': 'Mozilla/50 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Referer': f'{PTT_BASE_URL}/bbs/{BOARD}/index.html'
}
COOKIES = {'over18': '1'}

# å»ºç«‹ CloudScraper
scraper = cloudscraper.create_scraper()

STOPWORDS = set([
    'http', 'https', 'com', 'tw', 'imgur', 'jpg', 'jpeg', 'png', 'gif', 
    'youtu', 'be', 'link', 'url', 'æ–°è', 'åœ–ç‰‡', 'é€£çµ', 'è¨˜è€…', 'å ±å°', 
    'å•é¡Œ', 'å¤§å®¶', 'ä¸€å€‹', 'ä»€éº¼', 'é€™æ¨£', 'å‡ºä¾†', 'æ²’æœ‰', 'å¯ä»¥', 'æ€éº¼'
])

if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

def log(msg):
    if DEBUG_MODE:
        print(f"[DEBUG] {msg}")

# --- 2. æ•¸æ“šè®€å–èˆ‡è¼”åŠ©å‡½å¼ (æ ¸å¿ƒä¿®æ”¹å€) ---
def update_author_history_index():
    print("ğŸ”„ æ­£åœ¨æ›´æ–°ä½œè€…æ­·å²æ•¸æ“šç´¢å¼•...")
    
    # 1. å–å¾—æ‰€æœ‰ CSV
    all_csv_files = glob.glob(os.path.join(DATA_DIR, '**', '*.csv'), recursive=True)
    
    # ğŸš¨ [ä¿®æ­£é‡é»] è·¯å¾‘æ¨™æº–åŒ–æ¯”å°ï¼Œç¢ºä¿éæ¿¾æ‰æ­·å²æª”
    # å°‡è¨­å®šæª”çš„è·¯å¾‘èˆ‡ glob æ‰¾å‡ºçš„è·¯å¾‘éƒ½è½‰ç‚ºçµ•å°è·¯å¾‘æˆ–æ¨™æº–æ ¼å¼ä¾†æ¯”å°
    history_file_abs = os.path.abspath(AUTHOR_HISTORY_FILE)
    
    # éæ¿¾é‚è¼¯ï¼šåªä¿ç•™ã€Œä¸æ˜¯ã€æ­·å²çµ±è¨ˆæª”çš„ CSV
    target_files = []
    for f in all_csv_files:
        if os.path.abspath(f) != history_file_abs:
            target_files.append(f)
            
    if not target_files:
        print("âš ï¸ ç„¡åŸå§‹æ­·å²è³‡æ–™å¯æ›´æ–°ã€‚")
        return {}


    df_list = []
    for f in all_csv_files:
        try:
            # åªè®€å–å¿…è¦æ¬„ä½åŠ é€Ÿ
            df = pd.read_csv(f, usecols=['Post_ID', 'author', 'real_push_score'])
            df_list.append(df)
        except:
            continue
    
    if not df_list:
        return {}

    full_df = pd.concat(df_list, ignore_index=True)
    
    # ğŸš¨ é—œéµå»é‡é‚è¼¯: åŒä¸€ç¯‡æ–‡ç« å–æœ€é«˜åˆ† (ä»£è¡¨æœ€çµ‚æˆç¸¾)
    unique_posts = full_df.sort_values('real_push_score', ascending=False).drop_duplicates(subset=['Post_ID'], keep='first')
    
    # è¨ˆç®—å¹³å‡
    author_stats = unique_posts.groupby('author').agg(
        raw_avg=('real_push_score', 'mean'),
        count=('Post_ID', 'count')
    ).reset_index()
    
    # 5. æ‡‰ç”¨è²å¼å¹³æ»‘ (Bayesian Smoothing)
    # C = 3, Global Mean = 6.02
    C = 3
    global_mean = unique_posts['real_push_score'].mean() # è‡ªå‹•è¨ˆç®—ç•¶å‰å…¨ç«™å¹³å‡
    
    author_stats['author_avg_push'] = (
        (C * global_mean) + (author_stats['count'] * author_stats['raw_avg'])
    ) / (C + author_stats['count'])
    
    # åªä¿ç•™éœ€è¦çš„æ¬„ä½å­˜æª”
    final_df = author_stats[['author', 'author_avg_push']]
    final_df.to_csv(AUTHOR_HISTORY_FILE, index=False, encoding='utf-8-sig')
    
    print(f"âœ… ä½œè€…æ­·å²ç´¢å¼•å·²æ›´æ–° (å«è²å¼å¹³æ»‘)ï¼Œå…¨ç«™å¹³å‡: {global_mean:.2f}")
    
    return final_df.set_index('author')['author_avg_push'].to_dict()

def load_author_history():
    """
    ğŸ†• ä¿®æ”¹åŠŸèƒ½: å„ªå…ˆè®€å–å¿«å–æª”æ¡ˆï¼Œè‹¥ç„¡å‰‡åŸ·è¡Œæ›´æ–°ã€‚
    é€™æ¨£å¯ä»¥å°‡è®€å–æ™‚é–“å¾ O(Nå€‹æª”æ¡ˆ) é™ä½åˆ° O(1å€‹æª”æ¡ˆ)ã€‚
    """
    if os.path.exists(AUTHOR_HISTORY_FILE):
        try:
            df = pd.read_csv(AUTHOR_HISTORY_FILE)
            return df.set_index('author')['author_avg_push'].to_dict()
        except Exception as e:
            print(f"âš ï¸ è®€å–æ­·å²ç´¢å¼•æª”å¤±æ•—: {e}ï¼Œå˜—è©¦é‡æ–°è¨ˆç®—...")
            return update_author_history_index()
    else:
        # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå‰‡åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„è¨ˆç®—
        return update_author_history_index()

def get_soup(url):
    time.sleep(random.uniform(2.0, 5.0)) 
    try:
        resp = scraper.get(url, headers=HEADERS, cookies=COOKIES, timeout=30)
        if resp.status_code == 200:
            return BeautifulSoup(resp.text, 'html.parser')
        elif resp.status_code == 403:
            log("âš ï¸ Cloudflare 403 Forbidden")
        return None
    except Exception as e:
        log(f"âŒ é€£ç·šéŒ¯èª¤: {e}")
        return None

def extract_key_phrases(text, topK=5):
    if not text or len(text) < 10: return ""
    keywords = jieba.analyse.textrank(text, topK=topK*2, withWeight=False, allowPOS=('n', 'ns', 'nt', 'nz', 'vn', 'v', 'eng', 'a', 'vg'))
    filtered = [k for k in keywords if k.lower() not in STOPWORDS and len(k) > 1]
    return " ; ".join(filtered[:topK])

def get_article_category(title):
    match = re.search(r'^\[(.*?)\]', title)
    return match.group(1).strip() if match else 'General'

def clean_article_content(soup):
    main_content = soup.find(id='main-content')
    if not main_content: return "", 0, 0, 0, 0, 0
    
    for cls in ['article-metaline', 'article-metaline-right', 'push']:
        for div in main_content.find_all('div', class_=cls): div.extract()
    for span in main_content.find_all('span', class_='f2'): span.extract()

    text = main_content.text.strip()
    sp_count = len(re.findall(r'[^\w\s\u4E00-\u9FFF]', text))
    links = len(main_content.find_all('a', href=True))
    
    return text, len(list(jieba.cut(text))), text.count('?'), text.count('!'), links, sp_count

# --- 3. æŠ“å–æ–‡ç« å…§æ–‡ ---

def get_article_content(url):
    try:
        resp = scraper.get(url, headers=HEADERS, cookies=COOKIES, timeout=30)
        if resp.status_code != 200: return None
    except: return None

    soup = BeautifulSoup(resp.text, 'html.parser')
    
    push_score, push_c, boo_c, arrow_c = 0, 0, 0, 0
    for p in soup.find_all('div', class_='push'):
        tag = p.find('span', class_='push-tag')
        if tag:
            t = tag.text.strip()
            if t == 'æ¨': push_score += 1; push_c += 1
            elif t == 'å™“': push_score -= 1; boo_c += 1
            elif t == 'â†’': arrow_c += 1
    
    meta = soup.find_all('span', class_='article-meta-value')
    if len(meta) < 4: return None
    
    try:
        post_time = datetime.strptime(meta[3].text.strip(), '%a %b %d %H:%M:%S %Y')
    except: post_time = datetime.now()

    clean_text, wc, qc, ec, lc, spc = clean_article_content(soup)

    return {
        'author': meta[0].text.split('(')[0].strip(),
        'title': meta[2].text.strip(),
        'post_time': post_time,
        'real_push_score': push_score,
        'push_count': push_c, 'boo_count': boo_c, 'arrow_count': arrow_c,
        'clean_text': clean_text,
        'content_word_count': wc, 'question_mark_count': qc, 'exclamation_mark_count': ec,
        'link_count': lc, 'special_char_count': spc
    }

# --- 4. ä¸»è¦çˆ¬èŸ²é‚è¼¯ ---

def run_snapshot(author_history_cache):
    current_time = datetime.now()
    crawl_time_str = current_time.strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{crawl_time_str}] å•Ÿå‹• V2 å¿«ç…§çˆ¬èŸ²ä»»å‹™ (ç‰¹å¾µè£œå®Œç‰ˆ)...")

    # è¨­å®šå›æº¯æ™‚é–“
    lookback_hours = REGULAR_LOOKBACK_HOURS
    time_threshold = current_time - timedelta(hours=lookback_hours)
    
    articles_data = []
    url = PTT_URL
    keep_scraping = True
    
    while keep_scraping:
        soup = get_soup(url)
        if not soup: break
            
        divs = soup.find_all('div', class_='r-ent')
        if not divs: break
        
        # è™•ç†ç½®åº•åˆ†éš”ç·š
        sep = soup.find('div', class_='r-list-sep')
        if sep:
            divs = sep.find_all_previous('div', class_='r-ent')
        
        # åè½‰é †åº (å¾æœ€æ–°é–‹å§‹)
        for div in divs[::-1]:
            try:
                link = div.find('a')
                if not link: continue
                
                href = link['href']
                article_url = PTT_BASE_URL + href
                
                # ğŸš¨ è£œå›: æŠ“å–åˆ—è¡¨ä¸Šçš„ nrec_tag (åˆ—è¡¨æ¨æ–‡æ•¸é¡¯ç¤º)
                nrec_node = div.find('div', class_='nrec')
                nrec_tag = nrec_node.get_text().strip() if nrec_node else ""
                
                # é€²å…¥å…§æ–‡
                details = get_article_content(article_url)
                if not details: continue
                
                # æ™‚é–“ç¯©é¸
                if details['post_time'] < time_threshold:
                    keep_scraping = False
                    break 
                
                # --- ç‰¹å¾µè¨ˆç®— ---
                post_time = details['post_time']
                life_mins = (current_time - post_time).total_seconds() / 60
                
                # æ¨æ–‡åŠ é€Ÿåº¦
                accel = details['real_push_score'] / life_mins if life_mins > 1 else details['real_push_score']
                accel = min(accel, CLIPPING_THRESHOLD)
                
                # ä½œè€…å¹³å‡ (å¾å¿«å–è®€å–)
                author_avg = author_history_cache.get(details['author'], 0.0)
                
                # æ™‚é–“é€±æœŸç‰¹å¾µ
                h = post_time.hour
                
                # ğŸš¨ è£œå›: æ¨å™“æ¯” (é¿å…é™¤ä»¥ 0ï¼Œè‹¥ç„¡å™“æ–‡çµ¦äºˆ 1000 ä½œç‚ºä¸Šé™)
                pb_ratio = details['push_count'] / details['boo_count'] if details['boo_count'] > 0 else 1000.0
                
                # ğŸš¨ è£œå›: é€£çµå¯†åº¦
                word_count = details['content_word_count']
                url_ratio = details['link_count'] / word_count if word_count > 0 else 0.0

                articles_data.append({
                    # è­˜åˆ¥è³‡è¨Š
                    'Post_ID': href.split('/')[-1].replace('.html', ''),
                    'source_board': BOARD,
                    'title': details['title'],
                    'url': article_url,
                    'author': details['author'],
                    'crawl_time': crawl_time_str,
                    'post_time': post_time.strftime('%Y-%m-%d %H:%M:%S'),
                    
                    # ğŸš¨ è£œå›: åˆ—è¡¨ç‰¹å¾µ
                    'nrec_tag': nrec_tag,  # ä¾‹å¦‚ "çˆ†", "XX", "10"
                    
                    # ğŸš¨ è£œå›: å…§å®¹åˆ†é¡èˆ‡æ¨™é¡Œé•·åº¦
                    'category': get_article_category(details['title']),
                    'title_char_count': len(details['title']),
                    
                    # ğŸš¨ è£œå›: ç™¼æ–‡å°æ™‚ (åŸå§‹æ•¸å€¼)
                    'post_hour': h,

                    # æ•¸æ“šçµ±è¨ˆ
                    'real_push_score': details['real_push_score'],
                    'push_count': details['push_count'],
                    'boo_count': details['boo_count'],
                    
                    # é€²éšç‰¹å¾µ
                    'life_minutes': round(life_mins, 2),
                    'push_acceleration': round(accel, 4),
                    'push_boo_ratio': round(pb_ratio, 4), # ğŸš¨ è£œå›
                    'author_avg_push': round(author_avg, 2),
                    
                    # å…§å®¹ç‰¹å¾µ
                    'content_word_count': word_count,
                    'content_url_ratio': round(url_ratio, 4), # ğŸš¨ è£œå›
                    'q_mark_density': round(details['question_mark_count']/(word_count or 1), 4),
                    'e_mark_density': round(details['exclamation_mark_count']/(word_count or 1), 4),
                    'key_phrases': extract_key_phrases(details['clean_text']),
                    
                    # æ™‚é–“é€±æœŸ (Sin/Cos)
                    'hour_sin': round(np.sin(2 * np.pi * h / 24), 4),
                    'hour_cos': round(np.cos(2 * np.pi * h / 24), 4),
                    'is_weekend': 1 if post_time.weekday() >= 5 else 0
                })
                
            except Exception as e:
                # log(f"è™•ç†æ–‡ç« éŒ¯èª¤: {e}") # è‹¥æœ‰å®šç¾© log å‡½å¼å¯ä½¿ç”¨
                continue
        
        if not keep_scraping: break
        
        # æ›é é‚è¼¯
        btn = soup.find('div', class_='btn-group btn-group-paging')
        prev = btn.find('a', string='â€¹ ä¸Šé ') if btn else None
        if prev and 'href' in prev.attrs:
            url = PTT_BASE_URL + prev['href']
        else:
            break

    # å­˜æª”é‚è¼¯ (ç¶­æŒä¸è®Š)
    if articles_data:
        df = pd.DataFrame(articles_data)
        date_str = current_time.strftime('%Y%m%d')
        target_dir = os.path.join(DATA_DIR, date_str)
        if not os.path.exists(target_dir): os.makedirs(target_dir)
        
        fname = os.path.join(target_dir, f"ptt_snapshot_v2_{current_time.strftime('%Y%m%d_%H%M')}.csv")
        df.to_csv(fname, index=False, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸå„²å­˜ {len(df)} ç­†è³‡æ–™è‡³ {fname}")
        return True
    else:
        print("âš ï¸ ç„¡æ–°è³‡æ–™")
        return False

if __name__ == '__main__':
    print(f"ğŸš€ PTT çˆ†ç´…é æ¸¬çˆ¬èŸ² V2 (å„ªåŒ–ç‰ˆ) å·²å•Ÿå‹•")
    print(f"é »ç‡: {INTERVAL_SECONDS/60} åˆ†é˜ | å›æº¯: {REGULAR_LOOKBACK_HOURS} å°æ™‚")
    
    # 1. ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œå…ˆå¼·åˆ¶æ›´æ–°ä¸€æ¬¡ä½œè€…æ­·å²æ•¸æ“š
    print("â³ åˆå§‹åŒ–ï¼šæ­£åœ¨å»ºç«‹ä½œè€…æ­·å²æ•¸æ“šåº«...")
    author_history_cache = update_author_history_index()
    
    loop_count = 0
    UPDATE_HISTORY_EVERY_N_LOOPS = 6 # è¨­å®šæ¯è·‘å¹¾æ¬¡è¿´åœˆå°±æ›´æ–°ä¸€æ¬¡æ­·å²æª” (ä¾‹å¦‚ 6æ¬¡ = 1å°æ™‚)

    while True:
        try:
            # 2. åŸ·è¡Œçˆ¬èŸ²ï¼Œå‚³å…¥ç›®å‰çš„æ­·å²æ•¸æ“š
            has_data = run_snapshot(author_history_cache)
            
            # 3. å®šæœŸæ›´æ–°æ­·å²æ•¸æ“š (éæ¯æ¬¡ï¼Œç¯€çœæ•ˆèƒ½)
            loop_count += 1
            if loop_count >= UPDATE_HISTORY_EVERY_N_LOOPS:
                print("ğŸ”„ å®šæœŸæ›´æ–°ä½œè€…æ­·å²æ•¸æ“š...")
                author_history_cache = update_author_history_index()
                loop_count = 0
            
            next_run = datetime.now() + timedelta(seconds=INTERVAL_SECONDS)
            print(f"ğŸ˜´ ä¼‘çœ ä¸­... ä¸‹æ¬¡åŸ·è¡Œ: {next_run.strftime('%H:%M:%S')}\n")
            time.sleep(INTERVAL_SECONDS)
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ åœæ­¢ã€‚")
            break
        except Exception as e:
            print(f"\nâŒ éŒ¯èª¤: {e}")
            time.sleep(60)