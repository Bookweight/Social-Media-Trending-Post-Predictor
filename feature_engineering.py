import pandas as pd
import numpy as np
import os
from collections import Counter
from datetime import datetime, timedelta
from sklearn.model_selection import KFold
from sklearn.preprocessing import MultiLabelBinarizer
from tqdm import tqdm

# --- 1. è¨­å®šèˆ‡å¸¸é‡ ---
DATA_ROOT = 'data'
# å‡è¨­æˆ‘å€‘å¾ç™¼æ–‡å¾Œ 60 åˆ†é˜çš„ real_push_score è®ŠåŒ–ä¾†å®šç¾©çˆ†ç´…ç¨‹åº¦
TARGET_DELTA_MINUTES = 60
# è¨­å®š One-Hot Encoding çš„ Top N é—œéµè©æ•¸é‡
TOP_N_KEYWORDS = 300 
# Target Encoding ç”¨çš„å¹³æ»‘ä¿‚æ•¸ (ç”¨æ–¼é˜²æ­¢ä½é »è©éåº¦ç·¨ç¢¼)
SMOOTHING_ALPHA = 100 
# ğŸš€ å„ªåŒ–: push_boo_ratio çš„æˆªæ–·ä¸Šé™ (å–ä»£é™¤ä»¥é›¶çš„ 1000.0 ä½”ä½ç¬¦)
MAX_PUSH_BOO_RATIO_CLIP = 500.0 
# ğŸš€ æ–°å¢: push_acceleration çš„æˆªæ–·ä¸Šé™ (é˜²æ­¢æ¥µç«¯åŠ é€Ÿå€¼)
MAX_PUSH_ACCELERATION_CLIP = 5.0 

# --- 2. æ•¸æ“šåŠ è¼‰èˆ‡é è™•ç†æ ¸å¿ƒåŠŸèƒ½ ---

def load_and_prepare_data(data_root):
    """
    å¾è³‡æ–™å¤¾çµæ§‹ä¸­è¼‰å…¥æ‰€æœ‰å¿«ç…§æ•¸æ“šï¼Œä¸¦å°‡å®ƒå€‘çµ„åˆã€æ¸…æ´—ã€‚
    æ­¤ç‰ˆæœ¬ç¢ºä¿è¼‰å…¥æ‰€æœ‰å¿…éœ€çš„æ¬„ä½ã€‚
    """
    all_data = []
    # éæ­· 'data' è³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰æ—¥æœŸå­è³‡æ–™å¤¾
    for date_folder in os.listdir(data_root):
        date_path = os.path.join(data_root, date_folder)
        if os.path.isdir(date_path):
            # éæ­·æ—¥æœŸè³‡æ–™å¤¾å…§çš„æ‰€æœ‰ CSV å¿«ç…§
            for filename in os.listdir(date_path):
                if filename.endswith('.csv'):
                    filepath = os.path.join(date_path, filename)
                    try:
                        df = pd.read_csv(filepath)
                        # ğŸš€ æ›´æ–°: ç¢ºä¿æ‰€æœ‰ç”¨æ–¼æ¸…æ´—å’Œç‰¹å¾µå·¥ç¨‹çš„æ¬„ä½éƒ½è¢«è¼‰å…¥
                        required_cols = ['Post_ID', 'crawl_time', 'post_time', 'real_push_score', 'key_phrases', 
                                         'push_count', 'boo_count', 'push_boo_ratio', 'push_acceleration', 'author_avg_push']
                        if not all(col in df.columns for col in required_cols):
                            continue

                        # è½‰æ›æ™‚é–“æ ¼å¼
                        df['crawl_time'] = pd.to_datetime(df['crawl_time'])
                        df['post_time'] = pd.to_datetime(df['post_time'])
                        df['snapshot_id'] = df['crawl_time'].astype(str)
                        all_data.append(df[required_cols + ['snapshot_id']])
                    except Exception as e:
                        print(f"Error loading {filepath}: {e}")
                        continue
                        
    if not all_data:
        return pd.DataFrame()

    full_df = pd.concat(all_data, ignore_index=True)
    # ç¢ºä¿æŒ‰ Post_ID å’Œæ™‚é–“æ’åºï¼Œå° Target è¨ˆç®—è‡³é—œé‡è¦
    full_df.sort_values(by=['Post_ID', 'crawl_time'], inplace=True, ignore_index=True)
    return full_df

def clean_data(df, max_ratio_clip, max_acceleration_clip):
    """
    æ¸…æ´—æ•¸æ“šä¸­çš„ç•°å¸¸å€¼ï¼špush_boo_ratio çš„é™¤ä»¥é›¶éŒ¯èª¤ã€push_acceleration çš„æ¥µç«¯å€¼ï¼Œ
    ä»¥åŠä½œè€…å¹³å‡æ¨æ–‡æ•¸å’Œé—œéµè©çš„ç¼ºå¤±å€¼ã€‚
    """
    print(f"  [æ¸…æ´—ä¸­] æ­£åœ¨è™•ç†æ•¸å€¼èˆ‡é—œéµè©ç•°å¸¸é»...")
    
    # --- 1. æ¸…æ´— push_boo_ratio (é™¤ä»¥é›¶éŒ¯èª¤è™•ç†) ---
    mask_zero_boo = (df['boo_count'] == 0)
    
    # Case A: push_count > 0 ä¸” boo_count = 0 (çœŸæ­£çš„é«˜æ¯”ç‡) -> æˆªæ–·
    mask_high_ratio = mask_zero_boo & (df['push_count'] > 0)
    df.loc[mask_high_ratio, 'push_boo_ratio'] = max_ratio_clip
    
    # Case B: push_count = 0 ä¸” boo_count = 0 (é›¶åˆ†æ¯ã€é›¶åˆ†å­) -> è¨­ç‚ºä¸­æ€§å€¼ 1.0
    mask_neutral_ratio = mask_zero_boo & (df['push_count'] == 0)
    df.loc[mask_neutral_ratio, 'push_boo_ratio'] = 1.0

    print(f"  [æ¸…æ´—] push_boo_ratio å·²è™•ç† {mask_zero_boo.sum()} ç­† boo_count=0 çš„è¨˜éŒ„ã€‚")
    
    # --- 2. æ¸…æ´— push_acceleration (æ¥µç«¯å€¼æˆªæ–·) ---
    print(f"  [æ¸…æ´—ä¸­] æ­£åœ¨è™•ç† push_acceleration çš„ç•°å¸¸å€¼ (æˆªæ–·ä¸Šé™: {max_acceleration_clip})...")
    # å°‡æ‰€æœ‰é«˜æ–¼ä¸Šé™çš„æ•¸å€¼è¨­ç‚ºä¸Šé™å€¼
    df['push_acceleration'] = df['push_acceleration'].clip(upper=max_acceleration_clip)
    
    # --- 3. è™•ç† author_avg_push (ç¼ºå¤±å€¼å¡«è£œ) ---
    global_author_avg_push_mean = df['author_avg_push'].mean()
    # ä½¿ç”¨å…¨å±€å‡å€¼å¡«è£œ NaN (æ–°ä½œè€…æˆ–çˆ¬å–å¤±æ•—)
    df['author_avg_push'] = df['author_avg_push'].fillna(global_author_avg_push_mean)
    print(f"  [æ¸…æ´—] author_avg_push çš„ NaN å·²ç”¨å…¨å±€å‡å€¼ {global_author_avg_push_mean:.2f} å¡«è£œã€‚")

    # --- 4. è™•ç† key_phrases (ç¼ºå¤±å€¼æ¨™è¨˜) ---
    # å°‡ NaN é—œéµè©æ›¿æ›ç‚ºå–®ä¸€æ¨™ç±¤ï¼Œé¿å…è¨Šæ¯ä¸Ÿå¤±
    df['key_phrases'] = df['key_phrases'].fillna('NO_KEYWORDS')
    print("  [æ¸…æ´—] key_phrases çš„ NaN å·²æ›¿æ›ç‚º 'NO_KEYWORDS' æ¨™ç±¤ã€‚")
    
    return df

def calculate_target_delta(df, delta_minutes):
    """
    ğŸš€ å„ªåŒ–: ä½¿ç”¨ groupby().apply() åœ¨çµ„å…§é€²è¡Œé«˜æ•ˆç¯©é¸ï¼Œå–ä»£åœ¨æ•´å€‹ DF ä¸Šé‡è¤‡æŸ¥è©¢ã€‚
    è¨ˆç®—æ–‡ç« åœ¨ 'delta_minutes' å¾Œçš„æ¨æ–‡åˆ†æ•¸å¢é‡ (Delta Push Score)ã€‚
    """
    print("  [å„ªåŒ–ä¸­] æ­£åœ¨è¨ˆç®—ç›®æ¨™è®Šé‡ (Delta Push Score)...")
    df['future_crawl_time'] = df['crawl_time'] + timedelta(minutes=delta_minutes)
    
    def compute_delta_for_post(group):
        """å°å–®ç¯‡æ–‡ç«  (Post_ID group) æ‡‰ç”¨æ­¤é‚è¼¯"""
        target_scores = []
        
        # éæ­·è©²æ–‡ç« çš„æ‰€æœ‰å¿«ç…§
        for _, row in group.iterrows():
            current_score = row['real_push_score']
            target_time = row['future_crawl_time']
            
            # åœ¨è©²æ–‡ç« çµ„ (group) å…§ï¼Œå°‹æ‰¾æœªä¾†æ‰€æœ‰æ™‚é–“ >= target_time çš„å¿«ç…§
            future_snapshots = group[group['crawl_time'] >= target_time]
            
            if not future_snapshots.empty:
                # æ¡ç”¨ T+delta_minutes å¾Œæ‰€æœ‰å¿«ç…§ä¸­çš„æœ€é«˜åˆ†æ•¸
                max_future_score = future_snapshots['real_push_score'].max()
                
                # è¨ˆç®— Delta Push Scoreï¼Œçµæœè‡³å°‘ç‚º 0
                delta_score = max(0, max_future_score - current_score)
                target_scores.append(delta_score)
            else:
                # ç„¡æ³•è¨ˆç®— Target (å¤ªæ–°çš„å¿«ç…§)
                target_scores.append(np.nan) 
        
        # è¿”å›ä¸€å€‹èˆ‡ group ç´¢å¼•å°é½Šçš„ Series
        return pd.Series(target_scores, index=group.index)

    # æ‡‰ç”¨è¨ˆç®—å‡½å¼åˆ°æ¯å€‹ Post_ID ç¾¤çµ„
    tqdm.pandas(desc="Calculating Target Delta")
    df['target'] = df.groupby('Post_ID', group_keys=False).progress_apply(compute_delta_for_post)

    # åˆªé™¤ç„¡æ³•è¨ˆç®— Target çš„è¨˜éŒ„
    df.dropna(subset=['target'], inplace=True)
    
    # è¨­ç½®æœ€çµ‚çš„ Target Score
    df['target_score'] = df['target'] 
    
    return df

# --- 3. é—œéµè©ç‰¹å¾µè½‰æ› ---

def get_global_vocabulary(df):
    """æå–ä¸¦è¨ˆç®—æ‰€æœ‰é—œéµè©çš„ç¸½é »ç‡ï¼Œç”¨æ–¼ç¨€ç–åŒ–è™•ç†ã€‚"""
    all_keywords = []
    # ç¢ºä¿ 'key_phrases' ä¸æ˜¯ NaNï¼Œä¸¦ç”¨åˆ†è™Ÿåˆ†éš”
    # çµ±ä¸€å°‡é—œéµè©è½‰ç‚ºåˆ—è¡¨ï¼Œæ–¹ä¾¿å¾ŒçºŒè™•ç†
    # æ³¨æ„: clean_data å·²ç¶“å°‡ NaN æ›¿æ›ç‚º 'NO_KEYWORDS'
    keyword_lists = df['key_phrases'].apply(lambda x: x.split(' ; '))
    for kw_list in keyword_lists:
        all_keywords.extend(kw_list)
    
    keyword_counts = Counter(all_keywords)
    # ç¯©é¸å‡ºå‡ºç¾æ¬¡æ•¸æœ€å¤šçš„ TOP_N_KEYWORDS
    most_common_keywords = {word for word, count in keyword_counts.most_common(TOP_N_KEYWORDS)}
    
    print(f"ç¸½è©å½™é‡: {len(keyword_counts)}ï¼Œé¸å– Top {TOP_N_KEYWORDS} é€²è¡Œ One-Hot ç·¨ç¢¼ã€‚")
    return most_common_keywords

def create_keyword_features(df, most_common_keywords):
    """
    å‰µå»ºå…©ç¨®é—œéµè©ç‰¹å¾µï¼šOne-Hot ç¨€ç–ç‰¹å¾µ å’Œ Target Encoding ç‰¹å¾µã€‚
    ğŸš€ å„ªåŒ–: ä½¿ç”¨ MultiLabelBinarizer é€²è¡Œ One-Hot Encodingã€‚
    """
    # -----------------------------------------------------------
    print("\n--- 3.1 é—œéµè© One-Hot Encoding (Top N, å‘é‡åŒ–) ---")
    
    # 1. æº–å‚™æ•¸æ“š: å°‡ key_phrases è½‰æ›ç‚ºåˆ—è¡¨ï¼Œä¸¦åªä¿ç•™ Top N é—œéµè©
    keyword_list_filtered = df['key_phrases'].apply(
        lambda x: [kw for kw in x.split(' ; ') if kw in most_common_keywords]
    )
    
    # 2. ä½¿ç”¨ MultiLabelBinarizer é€²è¡Œå‘é‡åŒ– One-Hot
    mlb = MultiLabelBinarizer()
    # å¿…é ˆå…ˆ fit å† transformï¼Œä¸¦ç¢ºä¿ç´¢å¼•æ˜¯å°é½Šçš„
    keyword_matrix = mlb.fit_transform(keyword_list_filtered)
    
    # å‰µå»ºæ–°çš„ DataFrameï¼Œä¸¦ç¢ºä¿æ¬„ä½åæ¸…æ™°
    keyword_df = pd.DataFrame(
        keyword_matrix, 
        columns=[f'KW_OH_{kw}' for kw in mlb.classes_], 
        index=df.index
    )
    
    # 3. åˆä½µå›ä¸» DataFrame
    df = pd.concat([df, keyword_df], axis=1)

    # -----------------------------------------------------------
    print("\n--- 3.2 é—œéµè© Target Encoding (æ­·å²ç†±åº¦, K-Fold å¹³æ»‘) ---")
    
    # Target Encoding æ¬„ä½åç¨±
    df['KW_Target_Encoded'] = np.nan
    # è¨ˆç®—å…¨å±€å¹³å‡ Target (å…ˆé©—çŸ¥è­˜)
    global_mean_target = df['target_score'].mean()
    
    print(f"å…¨å±€å¹³å‡ Target Score: {global_mean_target:.4f}")

    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    # è¼”åŠ©å‡½å¼: å°‡ key_phrases è½‰æ›ç‚ºé—œéµè©åˆ—è¡¨ (åœ¨è¿´åœˆå¤–ä¸€æ¬¡è™•ç†)
    df['keywords_list'] = df['key_phrases'].apply(lambda x: x.split(' ; '))


    for fold, (train_index, val_index) in enumerate(kf.split(df)):
        df_train = df.iloc[train_index]
        
        # 1. åœ¨è¨“ç·´é›†ä¸Šå»ºç«‹ Keyword -> Target æ˜ å°„
        
        # ğŸš€ å„ªåŒ–: ä½¿ç”¨ explode/groupby/agg é€²è¡Œ Target çµ±è¨ˆ
        temp_train = df_train[['keywords_list', 'target_score']].explode('keywords_list')
        kw_stats = temp_train.groupby('keywords_list')['target_score'].agg(['count', 'mean']).reset_index()
        
        encoded_dict = {}
        for _, row in kw_stats.iterrows():
            kw = row['keywords_list']
            count = row['count']
            mean_target = row['mean']
            
            # Target Encoding å…¬å¼: (Count * Mean + Alpha * Global_Mean) / (Count + Alpha)
            smoothed_mean = (count * mean_target + SMOOTHING_ALPHA * global_mean_target) / (count + SMOOTHING_ALPHA)
            encoded_dict[kw] = smoothed_mean

        # 2. æ‡‰ç”¨æ˜ å°„åˆ°é©—è­‰é›† (é¿å…æ•¸æ“šæ´©éœ²)
        def apply_encoding_optimized(keywords_list):
            if not keywords_list: return global_mean_target
            # æ–‡ç« çš„ Target Encoding å–å…¶æ‰€æœ‰é—œéµè©çš„å¹³å‡ Target Score
            scores = [encoded_dict.get(kw, global_mean_target) for kw in keywords_list]
            return np.mean(scores)

        # æ‡‰ç”¨åˆ°é©—è­‰é›†
        df.loc[val_index, 'KW_Target_Encoded'] = df.iloc[val_index]['keywords_list'].apply(apply_encoding_optimized)
        
    # æ¸…ç†è‡¨æ™‚æ¬„ä½
    df.drop(columns=['keywords_list'], inplace=True)
    
    return df

# --- 4. åŸ·è¡Œæµç¨‹ ---
if __name__ == '__main__':
    # æ¨¡æ“¬æ•¸æ“šåŠ è¼‰
    print(f"--- 1. è¼‰å…¥åŸå§‹å¿«ç…§æ•¸æ“š (å¾ {DATA_ROOT} è³‡æ–™å¤¾)... ---")
    
    # ç‚ºäº†è®“è…³æœ¬èƒ½é‹è¡Œï¼Œæˆ‘å€‘åœ¨æ²’æœ‰å®Œæ•´è³‡æ–™å¤¾çµæ§‹æ™‚ï¼Œå…ˆä½¿ç”¨æä¾›çš„å–®ä¸€CSVä½œç‚ºæ¨¡æ“¬æ•¸æ“š
    if not os.path.exists(DATA_ROOT) or not any(os.path.isdir(os.path.join(DATA_ROOT, d)) for d in os.listdir(DATA_ROOT)):
        print("ğŸš¨ è­¦å‘Š: ç¼ºå°‘ data/æ—¥æœŸ/ çµæ§‹ã€‚ä½¿ç”¨å–®ä¸€ä¸Šå‚³æª”æ¡ˆé€²è¡Œæ¨¡æ“¬ã€‚")
        try:
            # ä½¿ç”¨ç”¨æˆ¶æä¾›çš„ ptt_snapshot_v2_20251127_0042.csv ä½œç‚ºæ¸¬è©¦æ•¸æ“š
            df = pd.read_csv('ptt_snapshot_v2_20251127_0042.csv')
            df['crawl_time'] = pd.to_datetime(df['crawl_time'])
            df['snapshot_id'] = df['crawl_time'].astype(str) # æ¨¡æ“¬ Group ID
        except FileNotFoundError:
            print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æä¾›çš„ CSV æª”æ¡ˆã€‚ç„¡æ³•ç¹¼çºŒã€‚")
            df = pd.DataFrame()
    else:
        df = load_and_prepare_data(DATA_ROOT)
        
    if df.empty:
        print("æ•¸æ“šåŠ è¼‰å¤±æ•—ï¼Œçµ‚æ­¢åŸ·è¡Œã€‚")
    else:
        print(f"ç¸½å…±è¼‰å…¥ {len(df)} ç­†å¿«ç…§è¨˜éŒ„ã€‚")
        
        # ğŸš€ åŸ·è¡Œæ•¸æ“šæ¸…æ´—
        df_cleaned = clean_data(df, MAX_PUSH_BOO_RATIO_CLIP, MAX_PUSH_ACCELERATION_CLIP)

        # Step 2: è¨ˆç®— Target (Delta Push Score)
        print(f"\n--- 2. è¨ˆç®—ç›®æ¨™è®Šé‡ (Delta Push Score @ T+{TARGET_DELTA_MINUTES}åˆ†é˜) ---")
        df_with_target = calculate_target_delta(df_cleaned, TARGET_DELTA_MINUTES)
        print(f"æˆåŠŸè¨ˆç®— Target çš„è¨˜éŒ„æ•¸: {len(df_with_target)}")

        # Step 3: æå–è©å½™è¡¨
        most_common_keywords = get_global_vocabulary(df_with_target)

        # Step 4: å‰µå»ºé—œéµè©ç‰¹å¾µ
        final_df = create_keyword_features(df_with_target, most_common_keywords)

        # é¡¯ç¤ºæœ€çµ‚ç‰¹å¾µçš„å½¢ç‹€å’Œéƒ¨åˆ†æ¬„ä½
        print("\n--- 4. é—œéµè©ç‰¹å¾µåŒ–çµæœ ---")
        print(f"æœ€çµ‚ DataFrame å½¢ç‹€: {final_df.shape}")
        
        feature_cols = [col for col in final_df.columns if col.startswith('KW_OH_') or col.startswith('KW_Target_')]
        print(f"ç”Ÿæˆçš„é—œéµè©ç‰¹å¾µæ¬„ä½æ•¸é‡: {len(feature_cols)}")
        print("éƒ¨åˆ†æ¬„ä½åç¨± (å‰3å€‹):")
        print(feature_cols[:3])
        print("\néƒ¨åˆ†çµæœå±•ç¤º (åŒ…å«æ¸…æ´—å¾Œçš„ push_boo_ratio, push_acceleration å’Œ Target Encoding ç‰¹å¾µ):")
        display_cols = ['Post_ID', 'crawl_time', 'key_phrases', 'push_boo_ratio', 'push_acceleration', 'author_avg_push', 'target_score', 'KW_Target_Encoded'] + [c for c in feature_cols if c.startswith('KW_OH_')][:3]
        print(final_df[display_cols].head())
        
        # --- 5. LGBM æ’åæ¨¡å‹æº–å‚™ ---
        
        # Group ID: æ’åæ¨¡å‹çš„é—œéµã€‚åœ¨åŒä¸€å€‹ 'snapshot_id' ä¸‹çš„æ–‡ç« é€²è¡Œæ’åã€‚
        final_df['group_id'] = final_df['snapshot_id']
        group_sizes = final_df.groupby('group_id').size().tolist()

        print("\n--- 5. LGBM è¨“ç·´æº–å‚™ ---")
        print("LGBM è¨“ç·´æ™‚æ‰€éœ€çš„æ ¸å¿ƒåƒæ•¸:")
        print(f"- ç‰¹å¾µ (X): åŒ…å« {len(feature_cols)} å€‹é—œéµè©ç‰¹å¾µ + å…¶ä»–æ•¸å€¼ç‰¹å¾µã€‚")
        print(f"- åˆ†çµ„ (group): {group_sizes[:5]}... (å‰ 5 çµ„çš„å¤§å°)")