import pandas as pd
import numpy as np
import glob
import os
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
import math
import gc
import re
from datetime import datetime, timedelta
import feature_utils

import db_manager

# --- è¨­å®šèˆ‡åƒæ•¸ ---
DATA_DIR = 'data'
MODEL_OUTPUT = 'ptt_lifecycle_model.txt'
LOOK_AHEAD_MINUTES = 120
TOLERANCE_MINUTES = 30
VELOCITY_DELTA_MINUTES = 10 

NUMERIC_COLS = feature_utils.NUMERIC_COLS
CAT_COLS = feature_utils.CAT_COLS

def parse_file_time(filepath):
    match = re.search(r'(\d{8}_\d{4})', filepath)
    if match:
        return datetime.strptime(match.group(1), '%Y%m%d_%H%M')
    return None

def load_recent_data(days_back=7):
    print(f"ğŸ“‚ æ­£åœ¨å¾è³‡æ–™åº«è®€å–æœ€è¿‘ {days_back} å¤©çš„è³‡æ–™...")
    
    end_time = datetime.now()
    start_time = end_time - timedelta(days=days_back)
    
    # ç›´æ¥ç”¨ SQL ç¯©é¸ï¼Œé€Ÿåº¦å¿«éå¸¸å¤š
    df = db_manager.query_snapshots_by_time_range(start_time, end_time)
    
    print(f"âœ… è¼‰å…¥å®Œæˆï¼ç¸½è³‡æ–™åˆ—æ•¸: {len(df)}")
    return df

def create_lifecycle_dataset(df):
    print(f"ğŸ”„ åŸ·è¡Œç”Ÿå‘½é€±æœŸé…å° (T+{LOOK_AHEAD_MINUTES}min)...")
    
    # 1. å»ºç«‹ã€Œæœªä¾†ã€æ¨™ç±¤ (T+60)
    df['target_lookup_time'] = df['crawl_time'] + pd.Timedelta(minutes=LOOK_AHEAD_MINUTES)
    df_future = df[['Post_ID', 'crawl_time', 'push_count', 'boo_count']].copy()
    df_future = df_future.sort_values('crawl_time')
    
    merged = pd.merge_asof(
        df,
        df_future,
        left_on='target_lookup_time',
        right_on='crawl_time',
        by='Post_ID',
        tolerance=pd.Timedelta(minutes=TOLERANCE_MINUTES),
        direction='nearest',
        suffixes=('', '_future')
    )
    
    # 2. å»ºç«‹ã€Œéå»ã€ç‰¹å¾µ (T-10)
    print(f"ğŸ”„ åŸ·è¡Œç¬æ™‚å‹•èƒ½é…å° (T-{VELOCITY_DELTA_MINUTES}min)...")
    merged['velocity_lookup_time'] = merged['crawl_time'] - pd.Timedelta(minutes=VELOCITY_DELTA_MINUTES)
    
    df_past = df[['Post_ID', 'crawl_time', 'push_count']].copy()
    df_past = df_past.sort_values('crawl_time')
    
    merged_final = pd.merge_asof(
        merged,
        df_past,
        left_on='velocity_lookup_time',
        right_on='crawl_time',
        by='Post_ID',
        tolerance=pd.Timedelta(minutes=TOLERANCE_MINUTES),
        direction='nearest',
        suffixes=('', '_prev') 
    )
    
    # ç§»é™¤é…å°å¤±æ•—çš„æ¨£æœ¬ (åªä¿ç•™æœ‰æœªä¾†çš„è³‡æ–™)
    valid_data = merged_final.dropna(subset=['push_count_future'])
    
    # é‡ç½®ç´¢å¼•ï¼Œé¿å…å¾ŒçºŒè™•ç†å‡ºéŒ¯
    valid_data = valid_data.reset_index(drop=True)
    
    del df, df_future, df_past, merged, merged_final
    gc.collect()
    
    return valid_data

def prepare_data_for_train(df):
    print("ğŸ› ï¸ æ­£åœ¨ç”Ÿæˆè¨“ç·´ç‰¹å¾µ...")
    
    # ğŸš¨ [ä¿®æ­£] ç›´æ¥å°‡åŒ…å« _prev æ¬„ä½çš„ df å‚³å…¥
    # feature_utils æœƒè‡ªå‹•åµæ¸¬ä¸¦ä½¿ç”¨é€™äº›æ¬„ä½ï¼Œä¸æœƒè§¸ç™¼ mergeï¼Œé¿å…çˆ†ç‚¸
    X = feature_utils.prepare_features_for_model(df, df_prev=None)
    
    # æ¨™ç±¤
    raw_score = df['push_count_future'] + df['boo_count_future']
    y = np.floor(5 * np.log1p(raw_score)).astype(int).clip(0, 30)
    
    # Group
    group = df.groupby('crawl_time', sort=False).size().to_list()
    
    return X, y, group

def run_training_pipeline(days_back=7):
    print("\n" + "="*50)
    print(f"ğŸ‹ï¸â€â™‚ï¸ å•Ÿå‹•æ¨¡å‹é‡è¨“æµç¨‹ (è³‡æ–™ç¯„åœ: è¿‘ {days_back} å¤©)")
    print("="*50)
    
    full_df = load_recent_data(days_back)
    if full_df.empty: return False
    
    dataset = create_lifecycle_dataset(full_df)
    if dataset.empty: return False

    n = len(dataset)
    train_end = int(n * 0.8)
    
    df_train = dataset.iloc[:train_end].copy()
    df_val = dataset.iloc[train_end:].copy()
    
    print(f"ğŸ“Š æ¨£æœ¬æ•¸: Train={len(df_train)}, Val={len(df_val)}")
    
    X_train, y_train, g_train = prepare_data_for_train(df_train)
    X_val, y_val, g_val = prepare_data_for_train(df_val)
    print("ğŸ§  é–‹å§‹è¨“ç·´ LightGBM (Full Retrain)...")
    
    # ğŸ†• å®šç¾©æ¬Šé‡éšæ¢¯ (0~30ç´š)
    custom_label_gain = [2**i - 1 for i in range(31)]

    gbm = lgb.LGBMRanker(
        objective='lambdarank',
        metric=['ndcg', 'map', 'rmse'],
        n_estimators=1000,
        learning_rate=0.05,
        num_leaves=31,
        random_state=42,
        importance_type='gain',
        lambdarank_truncation_level=10, # åªå°ˆæ³¨å„ªåŒ–å‰ 10 å
        label_gain=custom_label_gain    # çµ¦äºˆçˆ†æ–‡æ¥µé«˜çš„æ¬Šé‡
    )
    
    gbm.fit(
        X_train, y_train,
        group=g_train,
        eval_set=[(X_val, y_val)],
        eval_group=[g_val],
        eval_at=[10],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, first_metric_only=True),
            lgb.log_evaluation(period=50)
        ]
    )
    
    gbm.booster_.save_model(MODEL_OUTPUT)
    print(f"ğŸ’¾ é‡è¨“å®Œæˆï¼æ¨¡å‹å·²å„²å­˜è‡³ {MODEL_OUTPUT}")
    
    # é¡¯ç¤ºæ–°ç‰¹å¾µçš„é‡è¦æ€§
    imp = pd.DataFrame({
        'feature': X_train.columns,
        'gain': gbm.feature_importances_
    }).sort_values('gain', ascending=False)
    print("\nğŸ† æ–°æ¨¡å‹ç‰¹å¾µé‡è¦æ€§ (Top 10):")
    print(imp.head(10))
    
    del X_train, y_train, X_val, y_val
    gc.collect()
    
    return True

if __name__ == "__main__":
    run_training_pipeline(days_back=7)